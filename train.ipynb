{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "215af4ea26864cd5",
   "metadata": {},
   "source": [
    "#### Training to align the adapter using VQAv2 dataset"
   ]
  },
  {
   "cell_type": "code",
   "id": "d9432bb7f685363c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T05:13:51.087295Z",
     "start_time": "2025-11-27T05:13:22.167591Z"
    }
   },
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "from torchvision.transforms import RandomResizedCrop, Compose, Normalize, ToTensor\n",
    "import evaluate\n",
    "from transformers import Blip2Config, Blip2VisionConfig, Blip2ForConditionalGeneration, Blip2QFormerConfig\n",
    "from transformers import AutoModel, AutoTokenizer, SwinModel, SwinConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import Blip2Processor, AutoImageProcessor, BlipImageProcessor\n",
    "from transformers import AddedToken\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import Counter\n",
    "import wandb"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\toolkit\\anaconda\\envs\\torch\\lib\\site-packages\\transformers\\utils\\generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "1c46d1fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T05:13:53.640062Z",
     "start_time": "2025-11-27T05:13:53.635037Z"
    }
   },
   "source": [
    "# Configure HuggingFace cache directories (change the base path if needed)\n",
    "HF_CACHE_BASE = os.environ.get(\"HF_CACHE_BASE\", r\"D:/cache/huggingface\")\n",
    "HF_CACHE_BASE = os.path.abspath(HF_CACHE_BASE)\n",
    "HF_DATASETS_CACHE = os.path.join(HF_CACHE_BASE, \"datasets\")\n",
    "HF_MODELS_CACHE = os.path.join(HF_CACHE_BASE, \"models\")\n",
    "\n",
    "for path in (HF_CACHE_BASE, HF_DATASETS_CACHE, HF_MODELS_CACHE):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "os.environ[\"HF_HOME\"] = HF_CACHE_BASE\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = HF_DATASETS_CACHE\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = HF_MODELS_CACHE\n",
    "os.environ[\"HF_HUB_CACHE\"] = HF_MODELS_CACHE\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "6247457f3f8898cf",
   "metadata": {},
   "source": [
    "#### Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "id": "d06cab65fa1efa9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T05:13:56.184692Z",
     "start_time": "2025-11-27T05:13:56.169717Z"
    }
   },
   "source": [
    "class VQADataset(torch.utils.data.Dataset):\n",
    "    \"\"\"VQA (v2) dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, dataset, processor):\n",
    "        self.dataset = dataset\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def _select_answer(self, answers, fallback=\"\"):\n",
    "        candidates = []\n",
    "        if isinstance(answers, dict):\n",
    "            if isinstance(answers.get(\"text\"), list):\n",
    "                candidates = answers[\"text\"]\n",
    "            elif isinstance(answers.get(\"answer\"), list):\n",
    "                candidates = answers[\"answer\"]\n",
    "            elif isinstance(answers.get(\"answers\"), list):\n",
    "                candidates = answers[\"answers\"]\n",
    "        elif isinstance(answers, list):\n",
    "            if answers and isinstance(answers[0], dict):\n",
    "                candidates = [a.get(\"text\") or a.get(\"answer\") for a in answers if a.get(\"text\") or a.get(\"answer\")]\n",
    "            else:\n",
    "                candidates = answers\n",
    "        elif isinstance(answers, str):\n",
    "            candidates = [answers]\n",
    "\n",
    "        candidates = [c for c in candidates if isinstance(c, str) and c]\n",
    "        if candidates:\n",
    "            # make sure the answer contains at most one string\n",
    "            return Counter(candidates).most_common(1)[0][0]\n",
    "        return fallback\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        question = item[\"question\"]\n",
    "        answer = self._select_answer(item.get(\"answers\"), item.get(\"multiple_choice_answer\", \"\"))\n",
    "        image = item[\"image\"]\n",
    "\n",
    "        #encoding和decoding的max_length必须一样,这里都是64.因为language_model的logits的seq_len就是max_length,同时计算loss会logits = logits[:, -labels.size(1) :, :]，这里max_length不一样截取后loss计算就会错\n",
    "        encoding = self.processor(\n",
    "            images=image,\n",
    "            text=question,\n",
    "            padding=\"max_length\",\n",
    "            max_length=64,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        labels = self.processor.tokenizer(\n",
    "            answer,\n",
    "            max_length=64,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        #only keep the first eos in labels, other eos would be replaced by ignored_index of loss function, so that the loss won't count them\n",
    "        label_ids = labels[\"input_ids\"].squeeze(0)\n",
    "        pad_token_id = self.processor.tokenizer.pad_token_id\n",
    "        eos_token_id = self.processor.tokenizer.eos_token_id\n",
    "        if eos_token_id is None:\n",
    "            eos_token_id = pad_token_id\n",
    "\n",
    "        if pad_token_id is not None and pad_token_id != eos_token_id:\n",
    "            label_ids[label_ids == pad_token_id] = -100\n",
    "\n",
    "        if eos_token_id is not None:\n",
    "            eos_positions = (label_ids == eos_token_id).nonzero(as_tuple=False)\n",
    "            if eos_positions.numel() > 0:\n",
    "                label_ids[eos_positions.flatten()] = -100\n",
    "                first_eos_idx = eos_positions[0].item()\n",
    "                label_ids[first_eos_idx] = eos_token_id\n",
    "\n",
    "        encoding = {k: v.squeeze(0) for k, v in encoding.items()}\n",
    "        encoding[\"labels\"] = label_ids\n",
    "        return encoding"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "11a635955102fcf8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T05:14:02.227801Z",
     "start_time": "2025-11-27T05:14:00.255355Z"
    }
   },
   "source": [
    "from datasets.download.download_config import DownloadConfig\n",
    "\n",
    "##################################      Creating Dataset and Dataloader     ##################################\n",
    "raw_dataset = load_dataset(\n",
    "    \"HuggingFaceM4/VQAv2\",\n",
    "    cache_dir=HF_DATASETS_CACHE,\n",
    "    trust_remote_code=True,\n",
    "     download_config= DownloadConfig(storage_options={\"timeout\": 3600})\n",
    ")\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\": raw_dataset[\"train\"],\n",
    "    \"test\": raw_dataset[\"test\"]\n",
    "})\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "71cb0b095fe21fe1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T05:14:03.383191Z",
     "start_time": "2025-11-27T05:14:03.355705Z"
    }
   },
   "source": [
    "print(dataset[\"train\"][0])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question_type': 'what is this', 'multiple_choice_answer': 'net', 'answers': [{'answer': 'net', 'answer_confidence': 'maybe', 'answer_id': 1}, {'answer': 'net', 'answer_confidence': 'yes', 'answer_id': 2}, {'answer': 'net', 'answer_confidence': 'yes', 'answer_id': 3}, {'answer': 'netting', 'answer_confidence': 'yes', 'answer_id': 4}, {'answer': 'net', 'answer_confidence': 'yes', 'answer_id': 5}, {'answer': 'net', 'answer_confidence': 'yes', 'answer_id': 6}, {'answer': 'mesh', 'answer_confidence': 'maybe', 'answer_id': 7}, {'answer': 'net', 'answer_confidence': 'yes', 'answer_id': 8}, {'answer': 'net', 'answer_confidence': 'yes', 'answer_id': 9}, {'answer': 'net', 'answer_confidence': 'yes', 'answer_id': 10}], 'image_id': 458752, 'answer_type': 'other', 'question_id': 458752000, 'question': 'What is this photo taken looking through?', 'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x480 at 0x1217BF7C940>}\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "166eb0e9958425dd",
   "metadata": {},
   "source": [
    "#### Model creation"
   ]
  },
  {
   "cell_type": "code",
   "id": "ed4c59f7e2a1b12c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T05:34:30.200218Z",
     "start_time": "2025-11-27T05:33:33.592877Z"
    }
   },
   "source": [
    "##################################  Model Creation  ##################################\n",
    "llm_id = \"meta-llama/Llama-3.2-3B\"\n",
    "#llm_id = \"meta-llama/Llama-2-7b-hf\"\n",
    "base_model_id = \"Salesforce/blip2-opt-2.7b\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#dtype = torch.float16 if device.type == \"cuda\" else torch.float32\n",
    "\"\"\"\n",
    "Q-Former: bfloat16\n",
    "Vision encoder: bfloat16\n",
    "LLM (LLaMA / Flan-T5): bfloat16\n",
    "Loss: fp32\n",
    "发现 q-former若使用float16,其outputs直接nan，不知为啥\n",
    "\"\"\"\n",
    "dtype = torch.float32\n",
    "vision_dtype  = torch.bfloat16  if device.type == \"cuda\" else torch.float32\n",
    "qformer_dtype = torch.bfloat16 if device.type == \"cuda\" else torch.float32\n",
    "llm_dtype     = torch.bfloat16 if device.type == \"cuda\" else torch.float32\n",
    "\n",
    "processor = Blip2Processor.from_pretrained(base_model_id, cache_dir=HF_MODELS_CACHE,  use_fast=False) #tokenizer=0.13.0 doesn't support use_fast=True\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(base_model_id, torch_dtype=dtype, cache_dir=HF_MODELS_CACHE)\n",
    "\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(llm_id, use_fast=False, cache_dir=HF_MODELS_CACHE)\n",
    "if llama_tokenizer.pad_token is None:\n",
    "    llama_tokenizer.pad_token = llama_tokenizer.eos_token\n",
    "\n",
    "image_token = AddedToken(\"<image>\", normalized=False, special=True)\n",
    "llama_tokenizer.add_tokens([image_token], special_tokens=True)\n",
    "processor.tokenizer = llama_tokenizer\n",
    "processor.tokenizer.pad_token = processor.tokenizer.eos_token\n",
    "processor.num_query_tokens = model.config.num_query_tokens\n",
    "\n",
    "llama_model = AutoModelForCausalLM.from_pretrained(llm_id, torch_dtype=llm_dtype)\n",
    "llama_model = llama_model.to(device, dtype=llm_dtype)\n",
    "\n",
    "hidden_in = model.language_projection.in_features\n",
    "hidden_out = llama_model.config.hidden_size\n",
    "if model.language_projection.out_features != hidden_out:\n",
    "    projection = nn.Linear(hidden_in, hidden_out, bias=False)\n",
    "    nn.init.xavier_uniform_(projection.weight)\n",
    "    model.language_projection = projection\n",
    "\n",
    "# 这里手动设置各子模块精度\n",
    "model.vision_model = model.vision_model.to(device=device, dtype=vision_dtype)   # ViT: fp16\n",
    "model.qformer = model.qformer.to(device=device, dtype=qformer_dtype)            # Q-Former: bf16\n",
    "model.language_projection = model.language_projection.to(device=device, dtype=qformer_dtype)\n",
    "\n",
    "model.language_model = llama_model\n",
    "model.config.text_config = llama_model.config\n",
    "model.config.pad_token_id = llama_tokenizer.pad_token_id\n",
    "model.config.bos_token_id = llama_tokenizer.bos_token_id\n",
    "model.config.eos_token_id = llama_tokenizer.eos_token_id\n",
    "model.language_model.resize_token_embeddings(len(processor.tokenizer))\n",
    "model.config.vocab_size = len(processor.tokenizer)\n",
    "model.config.image_token_index = processor.tokenizer.convert_tokens_to_ids(\"<image>\")\n",
    "\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "for param in model.vision_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in model.language_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in model.qformer.parameters(): #freeze q-former\n",
    "    param.requires_grad = False\n",
    "\n",
    "# freeze query tokens\n",
    "model.query_tokens.requires_grad = False\n",
    "\n",
    "for param in model.language_projection.parameters():\n",
    "    param.requires_grad = True"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\toolkit\\anaconda\\envs\\torch\\lib\\site-packages\\huggingface_hub\\file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "783ef6bb6bb54e7a83d53f99aa8240b7"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "92c962b588874470b6a237a7c9a97308"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 32001. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "eca4017c45df1824",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T05:35:29.967449Z",
     "start_time": "2025-11-27T05:35:29.952815Z"
    }
   },
   "source": [
    "print(\"Model mixed-precision layout:\")\n",
    "\n",
    "def describe_module(module, name):\n",
    "    param_dtypes = {p.dtype for p in module.parameters()}\n",
    "    param_devices = {p.device for p in module.parameters()}\n",
    "    buffers = list(module.buffers())\n",
    "    buffer_dtypes = {b.dtype for b in buffers} if buffers else {\"<none>\"}\n",
    "    buffer_devices = {b.device for b in buffers} if buffers else {\"<none>\"}\n",
    "    print(f\"- {name}: params {param_dtypes} on {param_devices}; buffers {buffer_dtypes} on {buffer_devices}\")\n",
    "\n",
    "describe_module(model.vision_model, \"Vision Encoder\")\n",
    "describe_module(model.qformer, \"Q-Former\")\n",
    "describe_module(model.language_projection, \"Language Projection\")\n",
    "describe_module(model.language_model, \"LLM\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model mixed-precision layout:\n",
      "- Vision Encoder: params {torch.bfloat16} on {device(type='cuda', index=0)}; buffers {'<none>'} on {'<none>'}\n",
      "- Q-Former: params {torch.bfloat16} on {device(type='cuda', index=0)}; buffers {'<none>'} on {'<none>'}\n",
      "- Language Projection: params {torch.bfloat16} on {device(type='cuda', index=0)}; buffers {'<none>'} on {'<none>'}\n",
      "- LLM: params {torch.bfloat16} on {device(type='cuda', index=0)}; buffers {torch.bfloat16} on {device(type='cuda', index=0)}\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T05:35:36.208842Z",
     "start_time": "2025-11-27T05:35:36.198328Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 检查哪些参数 require_grad=True\n",
    "trainable = [n for n,p in model.named_parameters() if p.requires_grad]\n",
    "print(\"trainable param count:\", len(trainable))\n",
    "for n in trainable:\n",
    "    print(n)"
   ],
   "id": "d0e17e64cf7f4add",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable param count: 1\n",
      "language_projection.weight\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-27T05:35:41.142295Z",
     "start_time": "2025-11-27T05:35:38.954501Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "     # 预测：转到 CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "\n",
    "    # 标签：把 -100 换回 eos_token_id 才能 decode\n",
    "    labels = labels.clone()\n",
    "    labels[labels == -100] = processor.tokenizer.eos_token_id\n",
    "    labels = labels.detach().cpu().numpy()\n",
    "\n",
    "    decoded_preds = processor.tokenizer.batch_decode(logits, skip_special_tokens=True)\n",
    "    decoded_labels = processor.tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    # Compute ROUGE scores\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "\n",
    "    # We can return ROUGE-1, ROUGE-2, and ROUGE-L as needed\n",
    "    return {\n",
    "        \"rouge1\": result[\"rouge1\"],\n",
    "        \"rouge2\": result[\"rouge2\"],\n",
    "        \"rougeL\": result[\"rougeL\"],\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = VQADataset(dataset=dataset[\"train\"],\n",
    "                          processor=processor)\n",
    "valid_dataset = VQADataset(dataset=dataset[\"test\"],\n",
    "                          processor=processor)\n",
    "\n",
    "batch_size = 32\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=1, shuffle=False, pin_memory=True) #can't do validation, answer is None in this split\n",
    "\n",
    "##################################      Training Arguements     ##################################\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9, last_epoch=-1)\n",
    "\n",
    "num_epochs = 10\n",
    "patience = 10\n",
    "min_eval_acc = float(\"-inf\")\n",
    "early_stopping_hook = 0\n",
    "tracking_information = []\n",
    "\n",
    "wandb_run = wandb.init(\n",
    "    project=\"blip2-vqa\",\n",
    "    config={\n",
    "        \"learning_rate\": optimizer.param_groups[0][\"lr\"],\n",
    "        \"batch_size\": batch_size,\n",
    "        \"max_length\": 64,\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"base_model\": base_model_id,\n",
    "        \"llm\": llm_id,\n",
    "    },\n",
    ")\n",
    "wandb.watch(model, log=\"gradients\", log_freq=200, log_graph=False)\n",
    "global_step = 0\n",
    "\n",
    "##################################      Model Training and Evaluation      ##################################\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    model.train()\n",
    "    train_acc = 0\n",
    "    train_tqdm = tqdm(range(len(train_dataloader)), desc=f'Epoch {epoch+1} - Training loss: 0.000 - Train Acc: 0.000', position=0)\n",
    "    for idx, batch in zip(train_tqdm, train_dataloader):\n",
    "        input_ids = batch.pop('input_ids').to(device)\n",
    "        pixel_values = batch.pop('pixel_values').to(device)\n",
    "        attention_masked = batch.pop('attention_mask').to(device)\n",
    "        labels = batch.pop('labels').to(device)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            pixel_values=pixel_values,\n",
    "            attention_mask=attention_masked,\n",
    "            labels=labels,\n",
    "        )\n",
    "\n",
    "        \"\"\"\n",
    "        模型各子模块用手动 dtype（ViT fp16、Q-Former & projection bf16、LLM bf16）后，Forward 得到的 logits 会跟最后一层（LLaMA）同 dtype，也就是 torch.bfloat16。\n",
    "        Hugging Face 的 Blip2ForConditionalGeneration 内部在计算 loss时，会把 logits 自动提到 torch.float32，再和 labels（int64）做 softmax + log prob。所以你看到的 outputs.loss 实际上是 fp32（因此 .item() 也是正常的 float）。\n",
    "        反向传播时，梯度的 dtype 跟对应参数一致：\n",
    "        Q-Former、language projection：bf16 梯度\n",
    "        Vision encoder（虽然冻结）/LLM：fp16 或 bf16，但它们 requires_grad=False 就不会计算梯度\n",
    "        Optimizer 更新的就是当前仍在训练的 bf16 参数，因此可以继续搭配 gradient clipping。\n",
    "        \"\"\"\n",
    "        loss = outputs.loss #这里会忽视labels中token_id=-100的\n",
    "        epoch_loss += loss.item()\n",
    "        logits = outputs.logits.argmax(dim=-1)\n",
    "\n",
    "        train_acc += compute_metrics((logits, labels))[\"rougeL\"]  # You can use rouge1, rouge2, or rougeL\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        #grad_norm是裁剪前的l2范数，是所有require_grad=True的参数的l2范数\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)\n",
    "        optimizer.step()\n",
    "        current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "        wandb.log(\n",
    "            {\n",
    "                \"train/loss\": loss.item(),\n",
    "                \"train/grad_norm\": grad_norm.item() if hasattr(grad_norm, \"item\") else grad_norm,\n",
    "                \"train/lr\": current_lr,\n",
    "                \"train/epoch\": epoch + 1,\n",
    "            },\n",
    "            step=global_step,\n",
    "        )\n",
    "        global_step += 1\n",
    "        train_tqdm.set_description(f'Epoch {epoch+1} - Training loss: {epoch_loss/(idx+1):.4f} - Train Acc: {train_acc/(idx+1):.4f}')\n",
    "        # Clear cache to avoid OOM\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    #per epoch evaluation\n",
    "    #model.eval()\n",
    "    #eval_loss = 0\n",
    "    #eval_acc = 0\n",
    "    #val_tqdm = tqdm(range(len(valid_dataloader)), desc=f'Epoch {epoch+1} - Eval loss: 0.000 - Eval Acc: 0.000')\n",
    "    #for idx, batch in zip(val_tqdm, valid_dataloader):\n",
    "    #    input_ids = batch.pop('input_ids').to(device)\n",
    "    #    pixel_values = batch.pop('pixel_values').to(device)\n",
    "    #    attention_masked = batch.pop('attention_mask').to(device)\n",
    "    #    labels = batch.pop('labels').to(device)\n",
    "#\n",
    "    #    with torch.no_grad():\n",
    "    #        outputs = model(\n",
    "    #            input_ids=input_ids,\n",
    "    #            pixel_values=pixel_values,\n",
    "    #            attention_mask=attention_masked,\n",
    "    #            labels=labels,\n",
    "    #        )\n",
    "#\n",
    "    #    loss = outputs.loss\n",
    "    #    eval_loss += loss.item()\n",
    "#\n",
    "    #    logits = outputs.logits.argmax(dim=-1)\n",
    "    #    eval_acc += compute_metrics((logits, labels))[\"rougeL\"]  # You can use rouge1, rouge2, or rougeL\n",
    "#\n",
    "    #    val_tqdm.set_description(f'Epoch {epoch+1} - Eval loss: {eval_loss/(idx+1):.4f} - Eval Acc: {eval_acc/(idx+1):.4f}')\n",
    "    #    # Clear cache to avoid OOM\n",
    "    #    torch.cuda.empty_cache()\n",
    "    #    torch.cuda.synchronize()\n",
    "\n",
    "    #tracking_information.append((epoch_loss/len(train_dataloader), eval_loss/len(valid_dataloader), optimizer.param_groups[0][\"lr\"]))\n",
    "   # print(\"Epoch: {} - Training loss: {} - Eval Loss: {} - LR: {}\".format(epoch+1, epoch_loss/len(train_dataloader), eval_loss/len(valid_dataloader), optimizer.param_groups[0][\"lr\"]))\n",
    "    avg_loss = epoch_loss / len(train_dataloader)\n",
    "    avg_acc = train_acc / len(train_dataloader)\n",
    "    print(\"Epoch: {} - Training loss: {}  - LR: {}\".format(epoch+1, avg_loss, optimizer.param_groups[0][\"lr\"]))\n",
    "    wandb.log(\n",
    "        {\n",
    "            \"train/epoch_loss\": avg_loss,\n",
    "            \"train/epoch_accuracy\": avg_acc,\n",
    "            \"epoch\": epoch + 1,\n",
    "        },\n",
    "        step=global_step,\n",
    "    )\n",
    "    scheduler.step()\n",
    "    eval_acc = avg_acc\n",
    "    if eval_acc > min_eval_acc:\n",
    "        model.save_pretrained(\"./models/blip2\", from_pt=True)\n",
    "        processor.save_pretrained(\"./models/blip2_processor\")\n",
    "        print(\"Saved model and processor\")\n",
    "        min_eval_acc = eval_acc\n",
    "        early_stopping_hook = 0\n",
    "    else:\n",
    "        early_stopping_hook += 1\n",
    "        if early_stopping_hook > patience:\n",
    "            break\n",
    "\n",
    "pickle.dump(tracking_information, open(\"tracking_information.pkl\", \"wb\"))\n",
    "wandb_run.finish()\n",
    "print(\"The finetuning process has done!\")"
   ],
   "id": "144ec3927d6df4fc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": "baaa9a6470a828727bbaf3c078856703"
     }
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">different-dawn-7</strong> at: <a href='https://wandb.ai/yitingchen1228-new-york-university/blip2-vqa/runs/31ukpa4g' target=\"_blank\">https://wandb.ai/yitingchen1228-new-york-university/blip2-vqa/runs/31ukpa4g</a><br> View project at: <a href='https://wandb.ai/yitingchen1228-new-york-university/blip2-vqa' target=\"_blank\">https://wandb.ai/yitingchen1228-new-york-university/blip2-vqa</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251127_002323-31ukpa4g\\logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": "1f713d2f5ecfe345b811bb82515fa31f"
     }
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.22.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>D:\\School\\IntroToLLM\\Project\\wandb\\run-20251127_003539-vfrf8p9r</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/yitingchen1228-new-york-university/blip2-vqa/runs/vfrf8p9r' target=\"_blank\">spring-yogurt-8</a></strong> to <a href='https://wandb.ai/yitingchen1228-new-york-university/blip2-vqa' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/yitingchen1228-new-york-university/blip2-vqa' target=\"_blank\">https://wandb.ai/yitingchen1228-new-york-university/blip2-vqa</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/yitingchen1228-new-york-university/blip2-vqa/runs/vfrf8p9r' target=\"_blank\">https://wandb.ai/yitingchen1228-new-york-university/blip2-vqa/runs/vfrf8p9r</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training loss: 0.000 - Train Acc: 0.000:   0%|          | 0/13868 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Float but found BFloat16",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[15], line 72\u001B[0m\n\u001B[0;32m     69\u001B[0m attention_masked \u001B[38;5;241m=\u001B[39m batch\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mattention_mask\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m     70\u001B[0m labels \u001B[38;5;241m=\u001B[39m batch\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabels\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m---> 72\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     73\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     74\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpixel_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpixel_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     75\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_masked\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     76\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlabels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlabels\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     77\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     79\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     80\u001B[0m \u001B[38;5;124;03m模型各子模块用手动 dtype（ViT fp16、Q-Former & projection bf16、LLM bf16）后，Forward 得到的 logits 会跟最后一层（LLaMA）同 dtype，也就是 torch.bfloat16。\u001B[39;00m\n\u001B[0;32m     81\u001B[0m \u001B[38;5;124;03mHugging Face 的 Blip2ForConditionalGeneration 内部在计算 loss时，会把 logits 自动提到 torch.float32，再和 labels（int64）做 softmax + log prob。所以你看到的 outputs.loss 实际上是 fp32（因此 .item() 也是正常的 float）。\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     85\u001B[0m \u001B[38;5;124;03mOptimizer 更新的就是当前仍在训练的 bf16 参数，因此可以继续搭配 gradient clipping。\u001B[39;00m\n\u001B[0;32m     86\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     87\u001B[0m loss \u001B[38;5;241m=\u001B[39m outputs\u001B[38;5;241m.\u001B[39mloss \u001B[38;5;66;03m#这里会忽视labels中token_id=-100的\u001B[39;00m\n",
      "File \u001B[1;32mD:\\toolkit\\anaconda\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1773\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1774\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1775\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\toolkit\\anaconda\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1781\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1782\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1783\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1784\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1785\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1786\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1788\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1789\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32mD:\\toolkit\\anaconda\\envs\\torch\\lib\\site-packages\\transformers\\models\\blip_2\\modeling_blip_2.py:1709\u001B[0m, in \u001B[0;36mBlip2ForConditionalGeneration.forward\u001B[1;34m(self, pixel_values, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, output_attentions, output_hidden_states, labels, return_dict)\u001B[0m\n\u001B[0;32m   1705\u001B[0m return_dict \u001B[38;5;241m=\u001B[39m return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39muse_return_dict\n\u001B[0;32m   1707\u001B[0m \u001B[38;5;66;03m# step 1: forward the images through the vision encoder,\u001B[39;00m\n\u001B[0;32m   1708\u001B[0m \u001B[38;5;66;03m# to get image embeddings of shape (batch_size, seq_len, hidden_size)\u001B[39;00m\n\u001B[1;32m-> 1709\u001B[0m vision_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvision_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1710\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpixel_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpixel_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1711\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1712\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1713\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1714\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1715\u001B[0m image_embeds \u001B[38;5;241m=\u001B[39m vision_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m   1717\u001B[0m \u001B[38;5;66;03m# step 2: forward the query tokens through the QFormer, using the image embeddings for cross-attention\u001B[39;00m\n",
      "File \u001B[1;32mD:\\toolkit\\anaconda\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1773\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1774\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1775\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\toolkit\\anaconda\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1781\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1782\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1783\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1784\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1785\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1786\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1788\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1789\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32mD:\\toolkit\\anaconda\\envs\\torch\\lib\\site-packages\\transformers\\models\\blip_2\\modeling_blip_2.py:550\u001B[0m, in \u001B[0;36mBlip2VisionModel.forward\u001B[1;34m(self, pixel_values, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[0;32m    546\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou have to specify pixel_values\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    548\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membeddings(pixel_values)\n\u001B[1;32m--> 550\u001B[0m encoder_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoder\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    551\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    552\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    553\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    554\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    555\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    557\u001B[0m last_hidden_state \u001B[38;5;241m=\u001B[39m encoder_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m    558\u001B[0m last_hidden_state \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpost_layernorm(last_hidden_state)\n",
      "File \u001B[1;32mD:\\toolkit\\anaconda\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1773\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1774\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1775\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\toolkit\\anaconda\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1781\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1782\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1783\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1784\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1785\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1786\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1788\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1789\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32mD:\\toolkit\\anaconda\\envs\\torch\\lib\\site-packages\\transformers\\models\\blip_2\\modeling_blip_2.py:489\u001B[0m, in \u001B[0;36mBlip2Encoder.forward\u001B[1;34m(self, inputs_embeds, attention_mask, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[0;32m    483\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mcheckpoint\u001B[38;5;241m.\u001B[39mcheckpoint(\n\u001B[0;32m    484\u001B[0m         create_custom_forward(encoder_layer),\n\u001B[0;32m    485\u001B[0m         hidden_states,\n\u001B[0;32m    486\u001B[0m         attention_mask,\n\u001B[0;32m    487\u001B[0m     )\n\u001B[0;32m    488\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 489\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[43mencoder_layer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    490\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    491\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    492\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    493\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    495\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m layer_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m    497\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m output_attentions:\n",
      "File \u001B[1;32mD:\\toolkit\\anaconda\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1773\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1774\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1775\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\toolkit\\anaconda\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1781\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1782\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1783\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1784\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1785\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1786\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1788\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1789\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32mD:\\toolkit\\anaconda\\envs\\torch\\lib\\site-packages\\transformers\\models\\blip_2\\modeling_blip_2.py:246\u001B[0m, in \u001B[0;36mBlip2EncoderLayer.forward\u001B[1;34m(self, hidden_states, attention_mask, output_attentions)\u001B[0m\n\u001B[0;32m    234\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    235\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[0;32m    236\u001B[0m \u001B[38;5;124;03m    hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    242\u001B[0m \u001B[38;5;124;03m        returned tensors for more detail.\u001B[39;00m\n\u001B[0;32m    243\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    244\u001B[0m residual \u001B[38;5;241m=\u001B[39m hidden_states\n\u001B[1;32m--> 246\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlayer_norm1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    247\u001B[0m hidden_states, attn_weights \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mself_attn(\n\u001B[0;32m    248\u001B[0m     hidden_states\u001B[38;5;241m=\u001B[39mhidden_states,\n\u001B[0;32m    249\u001B[0m     head_mask\u001B[38;5;241m=\u001B[39mattention_mask,\n\u001B[0;32m    250\u001B[0m     output_attentions\u001B[38;5;241m=\u001B[39moutput_attentions,\n\u001B[0;32m    251\u001B[0m )\n\u001B[0;32m    252\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m hidden_states \u001B[38;5;241m+\u001B[39m residual\n",
      "File \u001B[1;32mD:\\toolkit\\anaconda\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1773\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1774\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1775\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\toolkit\\anaconda\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1781\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1782\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1783\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1784\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1785\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1786\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1788\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1789\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "Cell \u001B[1;32mIn[12], line 70\u001B[0m, in \u001B[0;36mBF16SafeLayerNorm.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     68\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m x\u001B[38;5;241m.\u001B[39mdtype \u001B[38;5;241m==\u001B[39m torch\u001B[38;5;241m.\u001B[39mbfloat16:\n\u001B[0;32m     69\u001B[0m     x \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mto(torch\u001B[38;5;241m.\u001B[39mfloat32)   \u001B[38;5;66;03m# 临时转 float32\u001B[39;00m\n\u001B[1;32m---> 70\u001B[0m y \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     71\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m orig_dtype \u001B[38;5;241m==\u001B[39m torch\u001B[38;5;241m.\u001B[39mbfloat16:\n\u001B[0;32m     72\u001B[0m     y \u001B[38;5;241m=\u001B[39m y\u001B[38;5;241m.\u001B[39mto(orig_dtype)     \u001B[38;5;66;03m# 再转回 bf16\u001B[39;00m\n",
      "File \u001B[1;32mD:\\toolkit\\anaconda\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\normalization.py:229\u001B[0m, in \u001B[0;36mLayerNorm.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    228\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 229\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlayer_norm\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    230\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnormalized_shape\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43meps\u001B[49m\n\u001B[0;32m    231\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\toolkit\\anaconda\\envs\\torch\\lib\\site-packages\\torch\\nn\\functional.py:2901\u001B[0m, in \u001B[0;36mlayer_norm\u001B[1;34m(input, normalized_shape, weight, bias, eps)\u001B[0m\n\u001B[0;32m   2891\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_variadic(\u001B[38;5;28minput\u001B[39m, weight, bias):\n\u001B[0;32m   2892\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m   2893\u001B[0m         layer_norm,\n\u001B[0;32m   2894\u001B[0m         (\u001B[38;5;28minput\u001B[39m, weight, bias),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   2899\u001B[0m         eps\u001B[38;5;241m=\u001B[39meps,\n\u001B[0;32m   2900\u001B[0m     )\n\u001B[1;32m-> 2901\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlayer_norm\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   2902\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnormalized_shape\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43meps\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackends\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcudnn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menabled\u001B[49m\n\u001B[0;32m   2903\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: expected scalar type Float but found BFloat16"
     ]
    }
   ],
   "execution_count": 15
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
