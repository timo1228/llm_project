{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "215af4ea26864cd5",
   "metadata": {},
   "source": [
    "#### Training to align the adapter using VQAv2 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9432bb7f685363c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T12:02:55.487657Z",
     "start_time": "2025-11-25T12:02:50.290338Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "from torchvision.transforms import RandomResizedCrop, Compose, Normalize, ToTensor\n",
    "import evaluate\n",
    "from transformers import Blip2Config, Blip2VisionConfig, Blip2ForConditionalGeneration, Blip2QFormerConfig\n",
    "from transformers import AutoModel, AutoTokenizer, SwinModel, SwinConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import Blip2Processor, AutoImageProcessor, BlipImageProcessor\n",
    "from transformers import AddedToken\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import Counter\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c46d1fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T12:03:09.275811Z",
     "start_time": "2025-11-25T12:03:09.268225Z"
    }
   },
   "outputs": [],
   "source": [
    "# Configure HuggingFace cache directories (change the base path if needed)\n",
    "HF_CACHE_BASE = os.environ.get(\"HF_CACHE_BASE\", r\"D:/cache/huggingface\")\n",
    "HF_CACHE_BASE = os.path.abspath(HF_CACHE_BASE)\n",
    "HF_DATASETS_CACHE = os.path.join(HF_CACHE_BASE, \"datasets\")\n",
    "HF_MODELS_CACHE = os.path.join(HF_CACHE_BASE, \"models\")\n",
    "\n",
    "for path in (HF_CACHE_BASE, HF_DATASETS_CACHE, HF_MODELS_CACHE):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "os.environ[\"HF_HOME\"] = HF_CACHE_BASE\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = HF_DATASETS_CACHE\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = HF_MODELS_CACHE\n",
    "os.environ[\"HF_HUB_CACHE\"] = HF_MODELS_CACHE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6247457f3f8898cf",
   "metadata": {},
   "source": [
    "#### Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d06cab65fa1efa9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T12:03:10.611170Z",
     "start_time": "2025-11-25T12:03:10.595966Z"
    }
   },
   "outputs": [],
   "source": [
    "class VQADataset(torch.utils.data.Dataset):\n",
    "    \"\"\"VQA (v2) dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, dataset, processor):\n",
    "        self.dataset = dataset\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def _select_answer(self, answers, fallback=\"\"):\n",
    "        candidates = []\n",
    "        if isinstance(answers, dict):\n",
    "            if isinstance(answers.get(\"text\"), list):\n",
    "                candidates = answers[\"text\"]\n",
    "            elif isinstance(answers.get(\"answer\"), list):\n",
    "                candidates = answers[\"answer\"]\n",
    "            elif isinstance(answers.get(\"answers\"), list):\n",
    "                candidates = answers[\"answers\"]\n",
    "        elif isinstance(answers, list):\n",
    "            if answers and isinstance(answers[0], dict):\n",
    "                candidates = [a.get(\"text\") or a.get(\"answer\") for a in answers if a.get(\"text\") or a.get(\"answer\")]\n",
    "            else:\n",
    "                candidates = answers\n",
    "        elif isinstance(answers, str):\n",
    "            candidates = [answers]\n",
    "\n",
    "        candidates = [c for c in candidates if isinstance(c, str) and c]\n",
    "        if candidates:\n",
    "            # make sure the answer contains at most one string\n",
    "            return Counter(candidates).most_common(1)[0][0]\n",
    "        return fallback\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        question = item[\"question\"]\n",
    "        answer = self._select_answer(item.get(\"answers\"), item.get(\"multiple_choice_answer\", \"\"))\n",
    "        image = item[\"image\"]\n",
    "\n",
    "        #encoding和decoding的max_length必须一样,这里都是64.因为language_model的logits的seq_len就是max_length,同时计算loss会logits = logits[:, -labels.size(1) :, :]，这里max_length不一样截取后loss计算就会错\n",
    "        encoding = self.processor(\n",
    "            images=image,\n",
    "            text=question,\n",
    "            padding=\"max_length\",\n",
    "            max_length=64,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        labels = self.processor.tokenizer(\n",
    "            answer,\n",
    "            max_length=64,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        #only keep the first eos in labels, other eos would be replaced by ignored_index of loss function, so that the loss won't count them\n",
    "        label_ids = labels[\"input_ids\"].squeeze(0)\n",
    "        pad_token_id = self.processor.tokenizer.pad_token_id\n",
    "        eos_token_id = self.processor.tokenizer.eos_token_id\n",
    "        if eos_token_id is None:\n",
    "            eos_token_id = pad_token_id\n",
    "\n",
    "        if pad_token_id is not None and pad_token_id != eos_token_id:\n",
    "            label_ids[label_ids == pad_token_id] = -100\n",
    "\n",
    "        if eos_token_id is not None:\n",
    "            eos_positions = (label_ids == eos_token_id).nonzero(as_tuple=False)\n",
    "            if eos_positions.numel() > 0:\n",
    "                label_ids[eos_positions.flatten()] = -100\n",
    "                first_eos_idx = eos_positions[0].item()\n",
    "                label_ids[first_eos_idx] = eos_token_id\n",
    "\n",
    "        encoding = {k: v.squeeze(0) for k, v in encoding.items()}\n",
    "        encoding[\"labels\"] = label_ids\n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11a635955102fcf8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T12:03:14.763640Z",
     "start_time": "2025-11-25T12:03:13.121174Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "from datasets.download.download_config import DownloadConfig\n",
    "\n",
    "##################################      Creating Dataset and Dataloader     ##################################\n",
    "raw_dataset = load_dataset(\n",
    "    \"HuggingFaceM4/VQAv2\",\n",
    "    cache_dir=HF_DATASETS_CACHE,\n",
    "    trust_remote_code=True,\n",
    "     download_config= DownloadConfig(storage_options={\"timeout\": 3600})\n",
    ")\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\": raw_dataset[\"train\"],\n",
    "    \"test\": raw_dataset[\"test\"]\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71cb0b095fe21fe1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T12:03:15.877710Z",
     "start_time": "2025-11-25T12:03:15.863709Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question_type': 'what is this', 'multiple_choice_answer': 'net', 'answers': [{'answer': 'net', 'answer_confidence': 'maybe', 'answer_id': 1}, {'answer': 'net', 'answer_confidence': 'yes', 'answer_id': 2}, {'answer': 'net', 'answer_confidence': 'yes', 'answer_id': 3}, {'answer': 'netting', 'answer_confidence': 'yes', 'answer_id': 4}, {'answer': 'net', 'answer_confidence': 'yes', 'answer_id': 5}, {'answer': 'net', 'answer_confidence': 'yes', 'answer_id': 6}, {'answer': 'mesh', 'answer_confidence': 'maybe', 'answer_id': 7}, {'answer': 'net', 'answer_confidence': 'yes', 'answer_id': 8}, {'answer': 'net', 'answer_confidence': 'yes', 'answer_id': 9}, {'answer': 'net', 'answer_confidence': 'yes', 'answer_id': 10}], 'image_id': 458752, 'answer_type': 'other', 'question_id': 458752000, 'question': 'What is this photo taken looking through?', 'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x480 at 0x2D58FB06620>}\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166eb0e9958425dd",
   "metadata": {},
   "source": [
    "#### Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed4c59f7e2a1b12c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T12:03:23.831101Z",
     "start_time": "2025-11-25T12:03:17.628306Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "652d457116b74f40b50c4f87ecbb7e64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e80eb04c2c8c49b8a1dfd18ac69696f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    }
   ],
   "source": [
    "##################################  Model Creation  ##################################\n",
    "llm_id = \"meta-llama/Llama-3.2-3B\"\n",
    "base_model_id = \"Salesforce/blip2-opt-2.7b\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#dtype = torch.float16 if device.type == \"cuda\" else torch.float32\n",
    "\"\"\"\n",
    "Q-Former: bfloat16\n",
    "Vision encoder: bfloat16\n",
    "LLM (LLaMA / Flan-T5): bfloat16\n",
    "Loss: fp32\n",
    "发现 q-former若使用float16,其outputs直接nan，不知为啥\n",
    "\"\"\"\n",
    "dtype = torch.float32\n",
    "vision_dtype  = torch.bfloat16  if device.type == \"cuda\" else torch.float32\n",
    "qformer_dtype = torch.bfloat16 if device.type == \"cuda\" else torch.float32\n",
    "llm_dtype     = torch.bfloat16 if device.type == \"cuda\" else torch.float32\n",
    "\n",
    "processor = Blip2Processor.from_pretrained(base_model_id, cache_dir=HF_MODELS_CACHE)\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(base_model_id, torch_dtype=dtype, cache_dir=HF_MODELS_CACHE)\n",
    "\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(llm_id, use_fast=False, cache_dir=HF_MODELS_CACHE)\n",
    "if llama_tokenizer.pad_token is None:\n",
    "    llama_tokenizer.pad_token = llama_tokenizer.eos_token\n",
    "\n",
    "image_token = AddedToken(\"<image>\", normalized=False, special=True)\n",
    "llama_tokenizer.add_tokens([image_token], special_tokens=True)\n",
    "processor.tokenizer = llama_tokenizer\n",
    "processor.tokenizer.pad_token = processor.tokenizer.eos_token\n",
    "processor.num_query_tokens = model.config.num_query_tokens\n",
    "\n",
    "llama_model = AutoModelForCausalLM.from_pretrained(llm_id, torch_dtype=llm_dtype)\n",
    "llama_model = llama_model.to(device, dtype=llm_dtype)\n",
    "\n",
    "hidden_in = model.language_projection.in_features\n",
    "hidden_out = llama_model.config.hidden_size\n",
    "if model.language_projection.out_features != hidden_out:\n",
    "    projection = nn.Linear(hidden_in, hidden_out, bias=False)\n",
    "    nn.init.xavier_uniform_(projection.weight)\n",
    "    model.language_projection = projection\n",
    "\n",
    "# 这里手动设置各子模块精度\n",
    "model.vision_model = model.vision_model.to(device=device, dtype=vision_dtype)   # ViT: fp16\n",
    "model.qformer = model.qformer.to(device=device, dtype=qformer_dtype)            # Q-Former: bf16\n",
    "model.language_projection = model.language_projection.to(device=device, dtype=qformer_dtype)\n",
    "\n",
    "model.language_model = llama_model\n",
    "model.config.text_config = llama_model.config\n",
    "model.config.pad_token_id = llama_tokenizer.pad_token_id\n",
    "model.config.bos_token_id = llama_tokenizer.bos_token_id\n",
    "model.config.eos_token_id = llama_tokenizer.eos_token_id\n",
    "model.language_model.resize_token_embeddings(len(processor.tokenizer))\n",
    "model.config.vocab_size = len(processor.tokenizer)\n",
    "model.config.image_token_index = processor.tokenizer.convert_tokens_to_ids(\"<image>\")\n",
    "model = model.to(device)\n",
    "\n",
    "for param in model.vision_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in model.language_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in model.qformer.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for param in model.language_projection.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eca4017c45df1824",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T12:03:26.899821Z",
     "start_time": "2025-11-25T12:03:26.879055Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model mixed-precision layout:\n",
      "- Vision Encoder: params {torch.bfloat16} on {device(type='cuda', index=0)}; buffers {'<none>'} on {'<none>'}\n",
      "- Q-Former: params {torch.bfloat16} on {device(type='cuda', index=0)}; buffers {'<none>'} on {'<none>'}\n",
      "- Language Projection: params {torch.bfloat16} on {device(type='cuda', index=0)}; buffers {'<none>'} on {'<none>'}\n",
      "- LLM: params {torch.bfloat16} on {device(type='cuda', index=0)}; buffers {torch.bfloat16} on {device(type='cuda', index=0)}\n"
     ]
    }
   ],
   "source": [
    "print(\"Model mixed-precision layout:\")\n",
    "\n",
    "def describe_module(module, name):\n",
    "    param_dtypes = {p.dtype for p in module.parameters()}\n",
    "    param_devices = {p.device for p in module.parameters()}\n",
    "    buffers = list(module.buffers())\n",
    "    buffer_dtypes = {b.dtype for b in buffers} if buffers else {\"<none>\"}\n",
    "    buffer_devices = {b.device for b in buffers} if buffers else {\"<none>\"}\n",
    "    print(f\"- {name}: params {param_dtypes} on {param_devices}; buffers {buffer_dtypes} on {buffer_devices}\")\n",
    "\n",
    "describe_module(model.vision_model, \"Vision Encoder\")\n",
    "describe_module(model.qformer, \"Q-Former\")\n",
    "describe_module(model.language_projection, \"Language Projection\")\n",
    "describe_module(model.language_model, \"LLM\")\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T12:20:26.508186Z",
     "start_time": "2025-11-25T12:20:26.172059Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 检查哪些参数 require_grad=True\n",
    "trainable = [n for n,p in model.named_parameters() if p.requires_grad]\n",
    "print(\"trainable param count:\", len(trainable))\n",
    "for n in trainable:\n",
    "    print(n)"
   ],
   "id": "d0e17e64cf7f4add",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable param count: 256\n",
      "query_tokens\n",
      "qformer.layernorm.weight\n",
      "qformer.layernorm.bias\n",
      "qformer.encoder.layer.0.attention.attention.query.weight\n",
      "qformer.encoder.layer.0.attention.attention.query.bias\n",
      "qformer.encoder.layer.0.attention.attention.key.weight\n",
      "qformer.encoder.layer.0.attention.attention.key.bias\n",
      "qformer.encoder.layer.0.attention.attention.value.weight\n",
      "qformer.encoder.layer.0.attention.attention.value.bias\n",
      "qformer.encoder.layer.0.attention.output.dense.weight\n",
      "qformer.encoder.layer.0.attention.output.dense.bias\n",
      "qformer.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "qformer.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "qformer.encoder.layer.0.crossattention.attention.query.weight\n",
      "qformer.encoder.layer.0.crossattention.attention.query.bias\n",
      "qformer.encoder.layer.0.crossattention.attention.key.weight\n",
      "qformer.encoder.layer.0.crossattention.attention.key.bias\n",
      "qformer.encoder.layer.0.crossattention.attention.value.weight\n",
      "qformer.encoder.layer.0.crossattention.attention.value.bias\n",
      "qformer.encoder.layer.0.crossattention.output.dense.weight\n",
      "qformer.encoder.layer.0.crossattention.output.dense.bias\n",
      "qformer.encoder.layer.0.crossattention.output.LayerNorm.weight\n",
      "qformer.encoder.layer.0.crossattention.output.LayerNorm.bias\n",
      "qformer.encoder.layer.0.intermediate_query.dense.weight\n",
      "qformer.encoder.layer.0.intermediate_query.dense.bias\n",
      "qformer.encoder.layer.0.output_query.dense.weight\n",
      "qformer.encoder.layer.0.output_query.dense.bias\n",
      "qformer.encoder.layer.0.output_query.LayerNorm.weight\n",
      "qformer.encoder.layer.0.output_query.LayerNorm.bias\n",
      "qformer.encoder.layer.1.attention.attention.query.weight\n",
      "qformer.encoder.layer.1.attention.attention.query.bias\n",
      "qformer.encoder.layer.1.attention.attention.key.weight\n",
      "qformer.encoder.layer.1.attention.attention.key.bias\n",
      "qformer.encoder.layer.1.attention.attention.value.weight\n",
      "qformer.encoder.layer.1.attention.attention.value.bias\n",
      "qformer.encoder.layer.1.attention.output.dense.weight\n",
      "qformer.encoder.layer.1.attention.output.dense.bias\n",
      "qformer.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "qformer.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "qformer.encoder.layer.1.intermediate_query.dense.weight\n",
      "qformer.encoder.layer.1.intermediate_query.dense.bias\n",
      "qformer.encoder.layer.1.output_query.dense.weight\n",
      "qformer.encoder.layer.1.output_query.dense.bias\n",
      "qformer.encoder.layer.1.output_query.LayerNorm.weight\n",
      "qformer.encoder.layer.1.output_query.LayerNorm.bias\n",
      "qformer.encoder.layer.2.attention.attention.query.weight\n",
      "qformer.encoder.layer.2.attention.attention.query.bias\n",
      "qformer.encoder.layer.2.attention.attention.key.weight\n",
      "qformer.encoder.layer.2.attention.attention.key.bias\n",
      "qformer.encoder.layer.2.attention.attention.value.weight\n",
      "qformer.encoder.layer.2.attention.attention.value.bias\n",
      "qformer.encoder.layer.2.attention.output.dense.weight\n",
      "qformer.encoder.layer.2.attention.output.dense.bias\n",
      "qformer.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "qformer.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "qformer.encoder.layer.2.crossattention.attention.query.weight\n",
      "qformer.encoder.layer.2.crossattention.attention.query.bias\n",
      "qformer.encoder.layer.2.crossattention.attention.key.weight\n",
      "qformer.encoder.layer.2.crossattention.attention.key.bias\n",
      "qformer.encoder.layer.2.crossattention.attention.value.weight\n",
      "qformer.encoder.layer.2.crossattention.attention.value.bias\n",
      "qformer.encoder.layer.2.crossattention.output.dense.weight\n",
      "qformer.encoder.layer.2.crossattention.output.dense.bias\n",
      "qformer.encoder.layer.2.crossattention.output.LayerNorm.weight\n",
      "qformer.encoder.layer.2.crossattention.output.LayerNorm.bias\n",
      "qformer.encoder.layer.2.intermediate_query.dense.weight\n",
      "qformer.encoder.layer.2.intermediate_query.dense.bias\n",
      "qformer.encoder.layer.2.output_query.dense.weight\n",
      "qformer.encoder.layer.2.output_query.dense.bias\n",
      "qformer.encoder.layer.2.output_query.LayerNorm.weight\n",
      "qformer.encoder.layer.2.output_query.LayerNorm.bias\n",
      "qformer.encoder.layer.3.attention.attention.query.weight\n",
      "qformer.encoder.layer.3.attention.attention.query.bias\n",
      "qformer.encoder.layer.3.attention.attention.key.weight\n",
      "qformer.encoder.layer.3.attention.attention.key.bias\n",
      "qformer.encoder.layer.3.attention.attention.value.weight\n",
      "qformer.encoder.layer.3.attention.attention.value.bias\n",
      "qformer.encoder.layer.3.attention.output.dense.weight\n",
      "qformer.encoder.layer.3.attention.output.dense.bias\n",
      "qformer.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "qformer.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "qformer.encoder.layer.3.intermediate_query.dense.weight\n",
      "qformer.encoder.layer.3.intermediate_query.dense.bias\n",
      "qformer.encoder.layer.3.output_query.dense.weight\n",
      "qformer.encoder.layer.3.output_query.dense.bias\n",
      "qformer.encoder.layer.3.output_query.LayerNorm.weight\n",
      "qformer.encoder.layer.3.output_query.LayerNorm.bias\n",
      "qformer.encoder.layer.4.attention.attention.query.weight\n",
      "qformer.encoder.layer.4.attention.attention.query.bias\n",
      "qformer.encoder.layer.4.attention.attention.key.weight\n",
      "qformer.encoder.layer.4.attention.attention.key.bias\n",
      "qformer.encoder.layer.4.attention.attention.value.weight\n",
      "qformer.encoder.layer.4.attention.attention.value.bias\n",
      "qformer.encoder.layer.4.attention.output.dense.weight\n",
      "qformer.encoder.layer.4.attention.output.dense.bias\n",
      "qformer.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "qformer.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "qformer.encoder.layer.4.crossattention.attention.query.weight\n",
      "qformer.encoder.layer.4.crossattention.attention.query.bias\n",
      "qformer.encoder.layer.4.crossattention.attention.key.weight\n",
      "qformer.encoder.layer.4.crossattention.attention.key.bias\n",
      "qformer.encoder.layer.4.crossattention.attention.value.weight\n",
      "qformer.encoder.layer.4.crossattention.attention.value.bias\n",
      "qformer.encoder.layer.4.crossattention.output.dense.weight\n",
      "qformer.encoder.layer.4.crossattention.output.dense.bias\n",
      "qformer.encoder.layer.4.crossattention.output.LayerNorm.weight\n",
      "qformer.encoder.layer.4.crossattention.output.LayerNorm.bias\n",
      "qformer.encoder.layer.4.intermediate_query.dense.weight\n",
      "qformer.encoder.layer.4.intermediate_query.dense.bias\n",
      "qformer.encoder.layer.4.output_query.dense.weight\n",
      "qformer.encoder.layer.4.output_query.dense.bias\n",
      "qformer.encoder.layer.4.output_query.LayerNorm.weight\n",
      "qformer.encoder.layer.4.output_query.LayerNorm.bias\n",
      "qformer.encoder.layer.5.attention.attention.query.weight\n",
      "qformer.encoder.layer.5.attention.attention.query.bias\n",
      "qformer.encoder.layer.5.attention.attention.key.weight\n",
      "qformer.encoder.layer.5.attention.attention.key.bias\n",
      "qformer.encoder.layer.5.attention.attention.value.weight\n",
      "qformer.encoder.layer.5.attention.attention.value.bias\n",
      "qformer.encoder.layer.5.attention.output.dense.weight\n",
      "qformer.encoder.layer.5.attention.output.dense.bias\n",
      "qformer.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "qformer.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "qformer.encoder.layer.5.intermediate_query.dense.weight\n",
      "qformer.encoder.layer.5.intermediate_query.dense.bias\n",
      "qformer.encoder.layer.5.output_query.dense.weight\n",
      "qformer.encoder.layer.5.output_query.dense.bias\n",
      "qformer.encoder.layer.5.output_query.LayerNorm.weight\n",
      "qformer.encoder.layer.5.output_query.LayerNorm.bias\n",
      "qformer.encoder.layer.6.attention.attention.query.weight\n",
      "qformer.encoder.layer.6.attention.attention.query.bias\n",
      "qformer.encoder.layer.6.attention.attention.key.weight\n",
      "qformer.encoder.layer.6.attention.attention.key.bias\n",
      "qformer.encoder.layer.6.attention.attention.value.weight\n",
      "qformer.encoder.layer.6.attention.attention.value.bias\n",
      "qformer.encoder.layer.6.attention.output.dense.weight\n",
      "qformer.encoder.layer.6.attention.output.dense.bias\n",
      "qformer.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "qformer.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "qformer.encoder.layer.6.crossattention.attention.query.weight\n",
      "qformer.encoder.layer.6.crossattention.attention.query.bias\n",
      "qformer.encoder.layer.6.crossattention.attention.key.weight\n",
      "qformer.encoder.layer.6.crossattention.attention.key.bias\n",
      "qformer.encoder.layer.6.crossattention.attention.value.weight\n",
      "qformer.encoder.layer.6.crossattention.attention.value.bias\n",
      "qformer.encoder.layer.6.crossattention.output.dense.weight\n",
      "qformer.encoder.layer.6.crossattention.output.dense.bias\n",
      "qformer.encoder.layer.6.crossattention.output.LayerNorm.weight\n",
      "qformer.encoder.layer.6.crossattention.output.LayerNorm.bias\n",
      "qformer.encoder.layer.6.intermediate_query.dense.weight\n",
      "qformer.encoder.layer.6.intermediate_query.dense.bias\n",
      "qformer.encoder.layer.6.output_query.dense.weight\n",
      "qformer.encoder.layer.6.output_query.dense.bias\n",
      "qformer.encoder.layer.6.output_query.LayerNorm.weight\n",
      "qformer.encoder.layer.6.output_query.LayerNorm.bias\n",
      "qformer.encoder.layer.7.attention.attention.query.weight\n",
      "qformer.encoder.layer.7.attention.attention.query.bias\n",
      "qformer.encoder.layer.7.attention.attention.key.weight\n",
      "qformer.encoder.layer.7.attention.attention.key.bias\n",
      "qformer.encoder.layer.7.attention.attention.value.weight\n",
      "qformer.encoder.layer.7.attention.attention.value.bias\n",
      "qformer.encoder.layer.7.attention.output.dense.weight\n",
      "qformer.encoder.layer.7.attention.output.dense.bias\n",
      "qformer.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "qformer.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "qformer.encoder.layer.7.intermediate_query.dense.weight\n",
      "qformer.encoder.layer.7.intermediate_query.dense.bias\n",
      "qformer.encoder.layer.7.output_query.dense.weight\n",
      "qformer.encoder.layer.7.output_query.dense.bias\n",
      "qformer.encoder.layer.7.output_query.LayerNorm.weight\n",
      "qformer.encoder.layer.7.output_query.LayerNorm.bias\n",
      "qformer.encoder.layer.8.attention.attention.query.weight\n",
      "qformer.encoder.layer.8.attention.attention.query.bias\n",
      "qformer.encoder.layer.8.attention.attention.key.weight\n",
      "qformer.encoder.layer.8.attention.attention.key.bias\n",
      "qformer.encoder.layer.8.attention.attention.value.weight\n",
      "qformer.encoder.layer.8.attention.attention.value.bias\n",
      "qformer.encoder.layer.8.attention.output.dense.weight\n",
      "qformer.encoder.layer.8.attention.output.dense.bias\n",
      "qformer.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "qformer.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "qformer.encoder.layer.8.crossattention.attention.query.weight\n",
      "qformer.encoder.layer.8.crossattention.attention.query.bias\n",
      "qformer.encoder.layer.8.crossattention.attention.key.weight\n",
      "qformer.encoder.layer.8.crossattention.attention.key.bias\n",
      "qformer.encoder.layer.8.crossattention.attention.value.weight\n",
      "qformer.encoder.layer.8.crossattention.attention.value.bias\n",
      "qformer.encoder.layer.8.crossattention.output.dense.weight\n",
      "qformer.encoder.layer.8.crossattention.output.dense.bias\n",
      "qformer.encoder.layer.8.crossattention.output.LayerNorm.weight\n",
      "qformer.encoder.layer.8.crossattention.output.LayerNorm.bias\n",
      "qformer.encoder.layer.8.intermediate_query.dense.weight\n",
      "qformer.encoder.layer.8.intermediate_query.dense.bias\n",
      "qformer.encoder.layer.8.output_query.dense.weight\n",
      "qformer.encoder.layer.8.output_query.dense.bias\n",
      "qformer.encoder.layer.8.output_query.LayerNorm.weight\n",
      "qformer.encoder.layer.8.output_query.LayerNorm.bias\n",
      "qformer.encoder.layer.9.attention.attention.query.weight\n",
      "qformer.encoder.layer.9.attention.attention.query.bias\n",
      "qformer.encoder.layer.9.attention.attention.key.weight\n",
      "qformer.encoder.layer.9.attention.attention.key.bias\n",
      "qformer.encoder.layer.9.attention.attention.value.weight\n",
      "qformer.encoder.layer.9.attention.attention.value.bias\n",
      "qformer.encoder.layer.9.attention.output.dense.weight\n",
      "qformer.encoder.layer.9.attention.output.dense.bias\n",
      "qformer.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "qformer.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "qformer.encoder.layer.9.intermediate_query.dense.weight\n",
      "qformer.encoder.layer.9.intermediate_query.dense.bias\n",
      "qformer.encoder.layer.9.output_query.dense.weight\n",
      "qformer.encoder.layer.9.output_query.dense.bias\n",
      "qformer.encoder.layer.9.output_query.LayerNorm.weight\n",
      "qformer.encoder.layer.9.output_query.LayerNorm.bias\n",
      "qformer.encoder.layer.10.attention.attention.query.weight\n",
      "qformer.encoder.layer.10.attention.attention.query.bias\n",
      "qformer.encoder.layer.10.attention.attention.key.weight\n",
      "qformer.encoder.layer.10.attention.attention.key.bias\n",
      "qformer.encoder.layer.10.attention.attention.value.weight\n",
      "qformer.encoder.layer.10.attention.attention.value.bias\n",
      "qformer.encoder.layer.10.attention.output.dense.weight\n",
      "qformer.encoder.layer.10.attention.output.dense.bias\n",
      "qformer.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "qformer.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "qformer.encoder.layer.10.crossattention.attention.query.weight\n",
      "qformer.encoder.layer.10.crossattention.attention.query.bias\n",
      "qformer.encoder.layer.10.crossattention.attention.key.weight\n",
      "qformer.encoder.layer.10.crossattention.attention.key.bias\n",
      "qformer.encoder.layer.10.crossattention.attention.value.weight\n",
      "qformer.encoder.layer.10.crossattention.attention.value.bias\n",
      "qformer.encoder.layer.10.crossattention.output.dense.weight\n",
      "qformer.encoder.layer.10.crossattention.output.dense.bias\n",
      "qformer.encoder.layer.10.crossattention.output.LayerNorm.weight\n",
      "qformer.encoder.layer.10.crossattention.output.LayerNorm.bias\n",
      "qformer.encoder.layer.10.intermediate_query.dense.weight\n",
      "qformer.encoder.layer.10.intermediate_query.dense.bias\n",
      "qformer.encoder.layer.10.output_query.dense.weight\n",
      "qformer.encoder.layer.10.output_query.dense.bias\n",
      "qformer.encoder.layer.10.output_query.LayerNorm.weight\n",
      "qformer.encoder.layer.10.output_query.LayerNorm.bias\n",
      "qformer.encoder.layer.11.attention.attention.query.weight\n",
      "qformer.encoder.layer.11.attention.attention.query.bias\n",
      "qformer.encoder.layer.11.attention.attention.key.weight\n",
      "qformer.encoder.layer.11.attention.attention.key.bias\n",
      "qformer.encoder.layer.11.attention.attention.value.weight\n",
      "qformer.encoder.layer.11.attention.attention.value.bias\n",
      "qformer.encoder.layer.11.attention.output.dense.weight\n",
      "qformer.encoder.layer.11.attention.output.dense.bias\n",
      "qformer.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "qformer.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "qformer.encoder.layer.11.intermediate_query.dense.weight\n",
      "qformer.encoder.layer.11.intermediate_query.dense.bias\n",
      "qformer.encoder.layer.11.output_query.dense.weight\n",
      "qformer.encoder.layer.11.output_query.dense.bias\n",
      "qformer.encoder.layer.11.output_query.LayerNorm.weight\n",
      "qformer.encoder.layer.11.output_query.LayerNorm.bias\n",
      "language_projection.weight\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "id": "144ec3927d6df4fc",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-25T12:32:58.856619Z",
     "start_time": "2025-11-25T12:20:58.275707Z"
    }
   },
   "source": [
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "     # 预测：转到 CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "\n",
    "    # 标签：把 -100 换回 eos_token_id 才能 decode\n",
    "    labels = labels.clone()\n",
    "    labels[labels == -100] = processor.tokenizer.eos_token_id\n",
    "    labels = labels.detach().cpu().numpy()\n",
    "\n",
    "    decoded_preds = processor.tokenizer.batch_decode(logits, skip_special_tokens=True)\n",
    "    decoded_labels = processor.tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    # Compute ROUGE scores\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "\n",
    "    # We can return ROUGE-1, ROUGE-2, and ROUGE-L as needed\n",
    "    return {\n",
    "        \"rouge1\": result[\"rouge1\"],\n",
    "        \"rouge2\": result[\"rouge2\"],\n",
    "        \"rougeL\": result[\"rougeL\"],\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = VQADataset(dataset=dataset[\"train\"],\n",
    "                          processor=processor)\n",
    "valid_dataset = VQADataset(dataset=dataset[\"test\"],\n",
    "                          processor=processor)\n",
    "\n",
    "batch_size = 32\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=1, shuffle=False, pin_memory=True) #can't do validation, answer is None in this split\n",
    "\n",
    "##################################      Training Arguements     ##################################\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9, last_epoch=-1)\n",
    "\n",
    "num_epochs = 10\n",
    "patience = 10\n",
    "min_eval_acc = float(\"-inf\")\n",
    "early_stopping_hook = 0\n",
    "tracking_information = []\n",
    "\n",
    "wandb_run = wandb.init(\n",
    "    project=\"blip2-vqa\",\n",
    "    config={\n",
    "        \"learning_rate\": optimizer.param_groups[0][\"lr\"],\n",
    "        \"batch_size\": batch_size,\n",
    "        \"max_length\": 64,\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"base_model\": base_model_id,\n",
    "        \"llm\": llm_id,\n",
    "    },\n",
    ")\n",
    "wandb.watch(model, log=\"gradients\", log_freq=200, log_graph=False)\n",
    "global_step = 0\n",
    "\n",
    "##################################      Model Training and Evaluation      ##################################\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    model.train()\n",
    "    train_acc = 0\n",
    "    train_tqdm = tqdm(range(len(train_dataloader)), desc=f'Epoch {epoch+1} - Training loss: 0.000 - Train Acc: 0.000', position=0)\n",
    "    for idx, batch in zip(train_tqdm, train_dataloader):\n",
    "        input_ids = batch.pop('input_ids').to(device)\n",
    "        pixel_values = batch.pop('pixel_values').to(device)\n",
    "        attention_masked = batch.pop('attention_mask').to(device)\n",
    "        labels = batch.pop('labels').to(device)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            pixel_values=pixel_values,\n",
    "            attention_mask=attention_masked,\n",
    "            labels=labels,\n",
    "        )\n",
    "\n",
    "        \"\"\"\n",
    "        模型各子模块用手动 dtype（ViT fp16、Q-Former & projection bf16、LLM bf16）后，Forward 得到的 logits 会跟最后一层（LLaMA）同 dtype，也就是 torch.bfloat16。\n",
    "        Hugging Face 的 Blip2ForConditionalGeneration 内部在计算 loss时，会把 logits 自动提到 torch.float32，再和 labels（int64）做 softmax + log prob。所以你看到的 outputs.loss 实际上是 fp32（因此 .item() 也是正常的 float）。\n",
    "        反向传播时，梯度的 dtype 跟对应参数一致：\n",
    "        Q-Former、language projection：bf16 梯度\n",
    "        Vision encoder（虽然冻结）/LLM：fp16 或 bf16，但它们 requires_grad=False 就不会计算梯度\n",
    "        Optimizer 更新的就是当前仍在训练的 bf16 参数，因此可以继续搭配 gradient clipping。\n",
    "        \"\"\"\n",
    "        loss = outputs.loss #这里会忽视labels中token_id=-100的\n",
    "        epoch_loss += loss.item()\n",
    "        logits = outputs.logits.argmax(dim=-1)\n",
    "\n",
    "        train_acc += compute_metrics((logits, labels))[\"rougeL\"]  # You can use rouge1, rouge2, or rougeL\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        #grad_norm是裁剪前的l2范数，是所有require_grad=True的参数的l2范数\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)\n",
    "        optimizer.step()\n",
    "        current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "        wandb.log(\n",
    "            {\n",
    "                \"train/loss\": loss.item(),\n",
    "                \"train/grad_norm\": grad_norm.item() if hasattr(grad_norm, \"item\") else grad_norm,\n",
    "                \"train/lr\": current_lr,\n",
    "                \"train/epoch\": epoch + 1,\n",
    "            },\n",
    "            step=global_step,\n",
    "        )\n",
    "        global_step += 1\n",
    "        train_tqdm.set_description(f'Epoch {epoch+1} - Training loss: {epoch_loss/(idx+1):.4f} - Train Acc: {train_acc/(idx+1):.4f}')\n",
    "        # Clear cache to avoid OOM\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    #per epoch evaluation\n",
    "    #model.eval()\n",
    "    #eval_loss = 0\n",
    "    #eval_acc = 0\n",
    "    #val_tqdm = tqdm(range(len(valid_dataloader)), desc=f'Epoch {epoch+1} - Eval loss: 0.000 - Eval Acc: 0.000')\n",
    "    #for idx, batch in zip(val_tqdm, valid_dataloader):\n",
    "    #    input_ids = batch.pop('input_ids').to(device)\n",
    "    #    pixel_values = batch.pop('pixel_values').to(device)\n",
    "    #    attention_masked = batch.pop('attention_mask').to(device)\n",
    "    #    labels = batch.pop('labels').to(device)\n",
    "#\n",
    "    #    with torch.no_grad():\n",
    "    #        outputs = model(\n",
    "    #            input_ids=input_ids,\n",
    "    #            pixel_values=pixel_values,\n",
    "    #            attention_mask=attention_masked,\n",
    "    #            labels=labels,\n",
    "    #        )\n",
    "#\n",
    "    #    loss = outputs.loss\n",
    "    #    eval_loss += loss.item()\n",
    "#\n",
    "    #    logits = outputs.logits.argmax(dim=-1)\n",
    "    #    eval_acc += compute_metrics((logits, labels))[\"rougeL\"]  # You can use rouge1, rouge2, or rougeL\n",
    "#\n",
    "    #    val_tqdm.set_description(f'Epoch {epoch+1} - Eval loss: {eval_loss/(idx+1):.4f} - Eval Acc: {eval_acc/(idx+1):.4f}')\n",
    "    #    # Clear cache to avoid OOM\n",
    "    #    torch.cuda.empty_cache()\n",
    "    #    torch.cuda.synchronize()\n",
    "\n",
    "    #tracking_information.append((epoch_loss/len(train_dataloader), eval_loss/len(valid_dataloader), optimizer.param_groups[0][\"lr\"]))\n",
    "   # print(\"Epoch: {} - Training loss: {} - Eval Loss: {} - LR: {}\".format(epoch+1, epoch_loss/len(train_dataloader), eval_loss/len(valid_dataloader), optimizer.param_groups[0][\"lr\"]))\n",
    "    avg_loss = epoch_loss / len(train_dataloader)\n",
    "    avg_acc = train_acc / len(train_dataloader)\n",
    "    print(\"Epoch: {} - Training loss: {}  - LR: {}\".format(epoch+1, avg_loss, optimizer.param_groups[0][\"lr\"]))\n",
    "    wandb.log(\n",
    "        {\n",
    "            \"train/epoch_loss\": avg_loss,\n",
    "            \"train/epoch_accuracy\": avg_acc,\n",
    "            \"epoch\": epoch + 1,\n",
    "        },\n",
    "        step=global_step,\n",
    "    )\n",
    "    scheduler.step()\n",
    "    eval_acc = avg_acc\n",
    "    if eval_acc > min_eval_acc:\n",
    "        model.save_pretrained(\"./models/blip2\", from_pt=True)\n",
    "        processor.save_pretrained(\"./models/blip2_processor\")\n",
    "        print(\"Saved model and processor\")\n",
    "        min_eval_acc = eval_acc\n",
    "        early_stopping_hook = 0\n",
    "    else:\n",
    "        early_stopping_hook += 1\n",
    "        if early_stopping_hook > patience:\n",
    "            break\n",
    "\n",
    "pickle.dump(tracking_information, open(\"tracking_information.pkl\", \"wb\"))\n",
    "wandb_run.finish()\n",
    "print(\"The finetuning process has done!\")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": "5018a9430fef79db1c11ddf5c29b8cc5"
     }
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/grad_norm</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▆▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/loss</td><td>█▆▁▆▆▃▄▄▃▆▃▄▂▄▄▃▁▂▄▃▃▂▃▃▃▇▃▄▅▃▁▂▄▁▃▃▃▃▂▂</td></tr><tr><td>train/lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/grad_norm</td><td>1.39106</td></tr><tr><td>train/loss</td><td>4</td></tr><tr><td>train/lr</td><td>0.0005</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">proud-salad-3</strong> at: <a href='https://wandb.ai/yitingchen1228-new-york-university/blip2-vqa/runs/td752yrn' target=\"_blank\">https://wandb.ai/yitingchen1228-new-york-university/blip2-vqa/runs/td752yrn</a><br> View project at: <a href='https://wandb.ai/yitingchen1228-new-york-university/blip2-vqa' target=\"_blank\">https://wandb.ai/yitingchen1228-new-york-university/blip2-vqa</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251125_070357-td752yrn\\logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": "d3cc52558bd98e83c09a3463a9f09409"
     }
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.22.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>D:\\School\\IntroToLLM\\Project\\wandb\\run-20251125_072058-ybq4ii2s</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/yitingchen1228-new-york-university/blip2-vqa/runs/ybq4ii2s' target=\"_blank\">rural-meadow-4</a></strong> to <a href='https://wandb.ai/yitingchen1228-new-york-university/blip2-vqa' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/yitingchen1228-new-york-university/blip2-vqa' target=\"_blank\">https://wandb.ai/yitingchen1228-new-york-university/blip2-vqa</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/yitingchen1228-new-york-university/blip2-vqa/runs/ybq4ii2s' target=\"_blank\">https://wandb.ai/yitingchen1228-new-york-university/blip2-vqa/runs/ybq4ii2s</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training loss: 3.8918 - Train Acc: 0.0063:   8%|▊         | 1057/13868 [11:58<2:25:08,  1.47it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[15], line 91\u001B[0m\n\u001B[0;32m     88\u001B[0m epoch_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n\u001B[0;32m     89\u001B[0m logits \u001B[38;5;241m=\u001B[39m outputs\u001B[38;5;241m.\u001B[39mlogits\u001B[38;5;241m.\u001B[39margmax(dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m---> 91\u001B[0m train_acc \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[43mcompute_metrics\u001B[49m\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlogits\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrougeL\u001B[39m\u001B[38;5;124m\"\u001B[39m]  \u001B[38;5;66;03m# You can use rouge1, rouge2, or rougeL\u001B[39;00m\n\u001B[0;32m     93\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m     94\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n",
      "Cell \u001B[1;32mIn[15], line 16\u001B[0m, in \u001B[0;36mcompute_metrics\u001B[1;34m(eval_pred)\u001B[0m\n\u001B[0;32m     14\u001B[0m decoded_labels \u001B[38;5;241m=\u001B[39m processor\u001B[38;5;241m.\u001B[39mtokenizer\u001B[38;5;241m.\u001B[39mbatch_decode(labels, skip_special_tokens\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m     15\u001B[0m \u001B[38;5;66;03m# Compute ROUGE scores\u001B[39;00m\n\u001B[1;32m---> 16\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[43mrouge\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompute\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpredictions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdecoded_preds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreferences\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdecoded_labels\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;66;03m# We can return ROUGE-1, ROUGE-2, and ROUGE-L as needed\u001B[39;00m\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m {\n\u001B[0;32m     20\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrouge1\u001B[39m\u001B[38;5;124m\"\u001B[39m: result[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrouge1\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[0;32m     21\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrouge2\u001B[39m\u001B[38;5;124m\"\u001B[39m: result[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrouge2\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[0;32m     22\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrougeL\u001B[39m\u001B[38;5;124m\"\u001B[39m: result[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrougeL\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[0;32m     23\u001B[0m }\n",
      "File \u001B[1;32mD:\\toolkit\\anaconda\\envs\\torch\\lib\\site-packages\\evaluate\\module.py:467\u001B[0m, in \u001B[0;36mEvaluationModule.compute\u001B[1;34m(self, predictions, references, **kwargs)\u001B[0m\n\u001B[0;32m    465\u001B[0m inputs \u001B[38;5;241m=\u001B[39m {input_name: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata[input_name][:] \u001B[38;5;28;01mfor\u001B[39;00m input_name \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_feature_names()}\n\u001B[0;32m    466\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m temp_seed(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mseed):\n\u001B[1;32m--> 467\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compute(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39minputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mcompute_kwargs)\n\u001B[0;32m    469\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuf_writer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    470\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuf_writer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\.cache\\huggingface\\modules\\evaluate_modules\\metrics\\evaluate-metric--rouge\\6e5315f72865c2eaa764c8361360bb938740b9c120a2cf3a7ad218aa0ce452ed\\rouge.py:149\u001B[0m, in \u001B[0;36mRouge._compute\u001B[1;34m(self, predictions, references, rouge_types, use_aggregator, use_stemmer, tokenizer)\u001B[0m\n\u001B[0;32m    146\u001B[0m         scores\u001B[38;5;241m.\u001B[39mappend(score)\n\u001B[0;32m    148\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m use_aggregator:\n\u001B[1;32m--> 149\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43maggregator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maggregate\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    150\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m result:\n\u001B[0;32m    151\u001B[0m         result[key] \u001B[38;5;241m=\u001B[39m result[key]\u001B[38;5;241m.\u001B[39mmid\u001B[38;5;241m.\u001B[39mfmeasure\n",
      "File \u001B[1;32mD:\\toolkit\\anaconda\\envs\\torch\\lib\\site-packages\\rouge_score\\scoring.py:124\u001B[0m, in \u001B[0;36mBootstrapAggregator.aggregate\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    122\u001B[0m score_matrix \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mvstack(\u001B[38;5;28mtuple\u001B[39m(scores))\n\u001B[0;32m    123\u001B[0m \u001B[38;5;66;03m# Percentiles are returned as (interval, measure).\u001B[39;00m\n\u001B[1;32m--> 124\u001B[0m percentiles \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_bootstrap_resample\u001B[49m\u001B[43m(\u001B[49m\u001B[43mscore_matrix\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    125\u001B[0m \u001B[38;5;66;03m# Extract the three intervals (low, mid, high).\u001B[39;00m\n\u001B[0;32m    126\u001B[0m intervals \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mtuple\u001B[39m(\n\u001B[0;32m    127\u001B[0m     (scores[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m(\u001B[38;5;241m*\u001B[39mpercentiles[j, :]) \u001B[38;5;28;01mfor\u001B[39;00m j \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m3\u001B[39m)))\n",
      "File \u001B[1;32mD:\\toolkit\\anaconda\\envs\\torch\\lib\\site-packages\\rouge_score\\scoring.py:149\u001B[0m, in \u001B[0;36mBootstrapAggregator._bootstrap_resample\u001B[1;34m(self, matrix)\u001B[0m\n\u001B[0;32m    147\u001B[0m sample_mean \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mzeros((\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_n_samples, matrix\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m]))\n\u001B[0;32m    148\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_n_samples):\n\u001B[1;32m--> 149\u001B[0m   sample_idx \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrandom\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mchoice\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    150\u001B[0m \u001B[43m      \u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43marange\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmatrix\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshape\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmatrix\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshape\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    151\u001B[0m   sample \u001B[38;5;241m=\u001B[39m matrix[sample_idx, :]\n\u001B[0;32m    152\u001B[0m   sample_mean[i, :] \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mmean(sample, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 15
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
