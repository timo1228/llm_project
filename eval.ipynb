{
 "cells": [
  {
   "cell_type": "code",
   "id": "58c0eeb3e99ba5fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T12:34:38.698439Z",
     "start_time": "2025-11-25T12:34:35.473586Z"
    }
   },
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    Blip2ForConditionalGeneration,\n",
    "    Blip2Processor,\n",
    ")\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "4ac30efacbb77891",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-25T12:34:46.173846Z",
     "start_time": "2025-11-25T12:34:40.055138Z"
    }
   },
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dtype = torch.float16 if device == \"cuda\" else torch.float32\n",
    "\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\n",
    "    \"./models/blip2\",\n",
    "    torch_dtype=dtype,\n",
    ")\n",
    "processor = Blip2Processor.from_pretrained(\"./models/blip2_processor\")\n",
    "model = model.to(device)\n",
    "print(\"Reloaded model from ./models/blip2\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0c0f6692e09e442fa2a4afbfc90bd5fd"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Blip2ForConditionalGeneration were not initialized from the model checkpoint at ./models/blip2 and are newly initialized: ['language_projection.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloaded model from ./models/blip2\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "1c623acf0459604b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T12:35:31.672050Z",
     "start_time": "2025-11-25T12:35:28.640577Z"
    }
   },
   "source": [
    "image = Image.open(\"./data/cats.jpeg\").convert(\"RGB\")\n",
    "prompt = \"Question: Describe the image. Answer:\"\n",
    "\n",
    "inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(device, dtype=dtype)\n",
    "print(inputs)\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "print(generated_ids)\n",
    "output = processor.decode(generated_ids[0], skip_special_tokens=True).strip()\n",
    "\n",
    "print(output)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[128256, 128256, 128256, 128256, 128256, 128256, 128256, 128256, 128256,\n",
      "         128256, 128256, 128256, 128256, 128256, 128256, 128256, 128256, 128256,\n",
      "         128256, 128256, 128256, 128256, 128256, 128256, 128256, 128256, 128256,\n",
      "         128256, 128256, 128256, 128256, 128256, 128000,  14924,     25,  61885,\n",
      "            279,   2217,     13,  22559,     25]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0'), 'pixel_values': tensor([[[[ 0.8501,  0.8501,  0.8501,  ..., -0.3032, -0.5366, -0.5806],\n",
      "          [ 0.8501,  0.8501,  0.8501,  ..., -0.3616, -0.5952, -0.6245],\n",
      "          [ 0.8501,  0.8501,  0.8501,  ..., -0.4492, -0.6680, -0.6538],\n",
      "          ...,\n",
      "          [ 1.3467,  1.3613,  1.3760,  ...,  0.2369,  0.2808,  0.2078],\n",
      "          [ 1.3906,  1.3760,  1.3760,  ...,  0.1639,  0.1201,  0.0471],\n",
      "          [ 1.4336,  1.4053,  1.3613,  ...,  0.1201, -0.0696, -0.3762]],\n",
      "\n",
      "         [[ 0.9492,  0.9492,  0.9492,  ..., -0.4014, -0.6416, -0.6865],\n",
      "          [ 0.9492,  0.9492,  0.9492,  ..., -0.4614, -0.6865, -0.7168],\n",
      "          [ 0.9492,  0.9492,  0.9492,  ..., -0.5215, -0.7466, -0.7314],\n",
      "          ...,\n",
      "          [ 1.3398,  1.3545,  1.3691,  ...,  0.0488,  0.0939,  0.0188],\n",
      "          [ 1.3848,  1.3691,  1.3691,  ..., -0.0262, -0.0712, -0.1462],\n",
      "          [ 1.4443,  1.4141,  1.3545,  ..., -0.0712, -0.2664, -0.5815]],\n",
      "\n",
      "         [[ 1.2783,  1.2783,  1.2783,  ..., -0.1720, -0.3994, -0.4421],\n",
      "          [ 1.2783,  1.2783,  1.2783,  ..., -0.2289, -0.4563, -0.4849],\n",
      "          [ 1.2783,  1.2783,  1.2783,  ..., -0.3000, -0.5132, -0.4990],\n",
      "          ...,\n",
      "          [ 1.3213,  1.3350,  1.3496,  ...,  0.1266,  0.1550,  0.0840],\n",
      "          [ 1.3496,  1.3496,  1.3496,  ...,  0.0698,  0.0271, -0.0582],\n",
      "          [ 1.3926,  1.3643,  1.3350,  ...,  0.0271, -0.1720, -0.4707]]]],\n",
      "       device='cuda:0', dtype=torch.float16)}\n",
      "tensor([[128256, 128256, 128256, 128256, 128256, 128256, 128256, 128256, 128256,\n",
      "         128256, 128256, 128256, 128256, 128256, 128256, 128256, 128256, 128256,\n",
      "         128256, 128256, 128256, 128256, 128256, 128256, 128256, 128256, 128256,\n",
      "         128256, 128256, 128256, 128256, 128256, 128000,  14924,     25,  61885,\n",
      "            279,   2217,     13,  22559,     25,  61885,    279,   2217,     13,\n",
      "          22559,     25,  61885,    279,   2217,     13,  22559,     25,  61885,\n",
      "            279,   2217,     13,  22559,     25,  61885,    279,   2217,     13,\n",
      "          22559,     25,  61885,    279,   2217,     13,  22559,     25,  61885,\n",
      "            279,   2217,     13,  22559,     25,  61885,    279,   2217,     13,\n",
      "          22559,     25,  61885,    279,   2217,     13,  22559,     25,  61885,\n",
      "            279,   2217,     13,  22559,     25,  61885,    279,   2217,     13,\n",
      "          22559,     25,  61885,    279,   2217,     13,  22559,     25,  61885,\n",
      "            279,   2217,     13,  22559,     25,  61885,    279,   2217,     13,\n",
      "          22559,     25,  61885,    279,   2217,     13,  22559,     25,  61885,\n",
      "            279,   2217,     13,  22559,     25,  61885,    279,   2217,     13,\n",
      "          22559,     25,  61885,    279,   2217,     13,  22559,     25,  61885,\n",
      "            279,   2217,     13,  22559,     25,  61885,    279,   2217,     13,\n",
      "          22559,     25,  61885,    279,   2217,     13,  22559,     25,  61885,\n",
      "            279,   2217,     13,  22559,     25,  61885,    279]],\n",
      "       device='cuda:0')\n",
      "Question: Describe the image. Answer: Describe the image. Answer: Describe the image. Answer: Describe the image. Answer: Describe the image. Answer: Describe the image. Answer: Describe the image. Answer: Describe the image. Answer: Describe the image. Answer: Describe the image. Answer: Describe the image. Answer: Describe the image. Answer: Describe the image. Answer: Describe the image. Answer: Describe the image. Answer: Describe the image. Answer: Describe the image. Answer: Describe the image. Answer: Describe the image. Answer: Describe the image. Answer: Describe the image. Answer: Describe the image. Answer: Describe the\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "a1c0cd9cdcca7092",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T12:36:25.607260Z",
     "start_time": "2025-11-25T12:36:25.402501Z"
    }
   },
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "from torchvision.transforms import RandomResizedCrop, Compose, Normalize, ToTensor\n",
    "import evaluate\n",
    "from transformers import Blip2Config, Blip2VisionConfig, Blip2ForConditionalGeneration, Blip2QFormerConfig\n",
    "from transformers import AutoModel, AutoTokenizer, SwinModel, SwinConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import Blip2Processor, AutoImageProcessor, BlipImageProcessor\n",
    "from transformers import AddedToken\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import Counter"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "155d00c7f8b2aac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T12:36:26.663063Z",
     "start_time": "2025-11-25T12:36:26.643588Z"
    }
   },
   "source": [
    "# Configure HuggingFace cache directories (change the base path if needed)\n",
    "HF_CACHE_BASE = os.environ.get(\"HF_CACHE_BASE\", r\"D:/cache/huggingface\")\n",
    "HF_CACHE_BASE = os.path.abspath(HF_CACHE_BASE)\n",
    "HF_DATASETS_CACHE = os.path.join(HF_CACHE_BASE, \"datasets\")\n",
    "HF_MODELS_CACHE = os.path.join(HF_CACHE_BASE, \"models\")\n",
    "\n",
    "for path in (HF_CACHE_BASE, HF_DATASETS_CACHE, HF_MODELS_CACHE):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "os.environ[\"HF_HOME\"] = HF_CACHE_BASE\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = HF_DATASETS_CACHE\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = HF_MODELS_CACHE\n",
    "os.environ[\"HF_HUB_CACHE\"] = HF_MODELS_CACHE"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "993ff8ab3bbc7429",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T12:36:27.787123Z",
     "start_time": "2025-11-25T12:36:27.772575Z"
    }
   },
   "source": [
    "class VQADataset(torch.utils.data.Dataset):\n",
    "    \"\"\"VQA (v2) dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, dataset, processor):\n",
    "        self.dataset = dataset\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def _select_answer(self, answers, fallback=\"\"):\n",
    "        candidates = []\n",
    "        if isinstance(answers, dict):\n",
    "            if isinstance(answers.get(\"text\"), list):\n",
    "                candidates = answers[\"text\"]\n",
    "            elif isinstance(answers.get(\"answer\"), list):\n",
    "                candidates = answers[\"answer\"]\n",
    "            elif isinstance(answers.get(\"answers\"), list):\n",
    "                candidates = answers[\"answers\"]\n",
    "        elif isinstance(answers, list):\n",
    "            if answers and isinstance(answers[0], dict):\n",
    "                candidates = [a.get(\"text\") or a.get(\"answer\") for a in answers if a.get(\"text\") or a.get(\"answer\")]\n",
    "            else:\n",
    "                candidates = answers\n",
    "        elif isinstance(answers, str):\n",
    "            candidates = [answers]\n",
    "\n",
    "        candidates = [c for c in candidates if isinstance(c, str) and c]\n",
    "        if candidates:\n",
    "            # make sure the answer contains at most one string\n",
    "            return Counter(candidates).most_common(1)[0][0]\n",
    "        return fallback\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        question = item[\"question\"]\n",
    "        answer = self._select_answer(item.get(\"answers\"), item.get(\"multiple_choice_answer\", \"\"))\n",
    "        image = item[\"image\"]\n",
    "\n",
    "        #encoding和decoding的max_length必须一样,这里都是64.因为language_model的logits的seq_len就是max_length,同时计算loss会logits = logits[:, -labels.size(1) :, :]，这里max_length不一样截取后loss计算就会错\n",
    "        encoding = self.processor(\n",
    "            images=image,\n",
    "            text=question,\n",
    "            padding=\"max_length\",\n",
    "            max_length=64,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        labels = self.processor.tokenizer(\n",
    "            answer,\n",
    "            max_length=64,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        #only keep the first eos in labels, other eos would be replaced by ignored_index of loss function, so that the loss won't count them\n",
    "        label_ids = labels[\"input_ids\"].squeeze(0)\n",
    "        pad_token_id = self.processor.tokenizer.pad_token_id\n",
    "        eos_token_id = self.processor.tokenizer.eos_token_id\n",
    "        if eos_token_id is None:\n",
    "            eos_token_id = pad_token_id\n",
    "\n",
    "        if pad_token_id is not None and pad_token_id != eos_token_id:\n",
    "            label_ids[label_ids == pad_token_id] = -100\n",
    "\n",
    "        if eos_token_id is not None:\n",
    "            eos_positions = (label_ids == eos_token_id).nonzero(as_tuple=False)\n",
    "            if eos_positions.numel() > 0:\n",
    "                label_ids[eos_positions.flatten()] = -100\n",
    "                first_eos_idx = eos_positions[0].item()\n",
    "                label_ids[first_eos_idx] = eos_token_id\n",
    "\n",
    "        encoding = {k: v.squeeze(0) for k, v in encoding.items()}\n",
    "        encoding[\"labels\"] = label_ids\n",
    "        return encoding"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "8af348184b754869",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T12:36:32.004720Z",
     "start_time": "2025-11-25T12:36:30.719252Z"
    }
   },
   "source": [
    "from datasets.download.download_config import DownloadConfig\n",
    "\n",
    "##################################      Creating Dataset and Dataloader    ##################################\n",
    "raw_dataset = load_dataset(\n",
    "    \"HuggingFaceM4/VQAv2\",\n",
    "    cache_dir=HF_DATASETS_CACHE,\n",
    "    trust_remote_code=True,\n",
    "     download_config= DownloadConfig(storage_options={\"timeout\": 3600})\n",
    ")\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\": raw_dataset[\"train\"],\n",
    "    \"test\": raw_dataset[\"test\"]\n",
    "})\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "8f5e35315ac0689b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T12:36:34.234075Z",
     "start_time": "2025-11-25T12:36:34.222763Z"
    }
   },
   "source": "print(dataset[\"train\"][0])",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question_type': 'what is this', 'multiple_choice_answer': 'net', 'answers': [{'answer': 'net', 'answer_confidence': 'maybe', 'answer_id': 1}, {'answer': 'net', 'answer_confidence': 'yes', 'answer_id': 2}, {'answer': 'net', 'answer_confidence': 'yes', 'answer_id': 3}, {'answer': 'netting', 'answer_confidence': 'yes', 'answer_id': 4}, {'answer': 'net', 'answer_confidence': 'yes', 'answer_id': 5}, {'answer': 'net', 'answer_confidence': 'yes', 'answer_id': 6}, {'answer': 'mesh', 'answer_confidence': 'maybe', 'answer_id': 7}, {'answer': 'net', 'answer_confidence': 'yes', 'answer_id': 8}, {'answer': 'net', 'answer_confidence': 'yes', 'answer_id': 9}, {'answer': 'net', 'answer_confidence': 'yes', 'answer_id': 10}], 'image_id': 458752, 'answer_type': 'other', 'question_id': 458752000, 'question': 'What is this photo taken looking through?', 'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x480 at 0x10F05622980>}\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "efe935b64cefc6f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T12:36:46.384960Z",
     "start_time": "2025-11-25T12:36:45.489337Z"
    }
   },
   "source": [
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "     # 预测：转到 CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "\n",
    "    # 标签：把 -100 换回 eos_token_id 才能 decode\n",
    "    labels = labels.clone()\n",
    "    labels[labels == -100] = processor.tokenizer.eos_token_id\n",
    "    labels = labels.detach().cpu().numpy()\n",
    "\n",
    "    decoded_preds = processor.tokenizer.batch_decode(logits, skip_special_tokens=True)\n",
    "    decoded_labels = processor.tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    # Compute ROUGE scores\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "\n",
    "    # We can return ROUGE-1, ROUGE-2, and ROUGE-L as needed\n",
    "    return {\n",
    "        \"rouge1\": result[\"rouge1\"],\n",
    "        \"rouge2\": result[\"rouge2\"],\n",
    "        \"rougeL\": result[\"rougeL\"],\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = VQADataset(dataset=dataset[\"train\"],\n",
    "                          processor=processor)\n",
    "valid_dataset = VQADataset(dataset=dataset[\"test\"],\n",
    "                          processor=processor)\n",
    "batch_size = 10\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=1, shuffle=False, pin_memory=True)\n",
    "loss_fct = nn.CrossEntropyLoss(ignore_index=-100, reduction=\"mean\")"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "8d818edf7f4b4450",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T12:38:10.282802Z",
     "start_time": "2025-11-25T12:36:50.656601Z"
    }
   },
   "source": [
    "model.eval()\n",
    "train_acc = 0\n",
    "train_tqdm = tqdm(range(len(train_dataloader)), desc=f'Epoch {1} - Training loss: 0.000 - Train Acc: 0.000', position=0)\n",
    "epoch_loss = 0\n",
    "for idx, batch in zip(train_tqdm, train_dataloader):\n",
    "    input_ids = batch.pop('input_ids').to(device)\n",
    "    pixel_values = batch.pop('pixel_values').to(device)\n",
    "    attention_masked = batch.pop('attention_mask').to(device)\n",
    "    labels = batch.pop('labels').to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            pixel_values=pixel_values,\n",
    "            attention_mask=attention_masked,\n",
    "            labels=labels,\n",
    "        )\n",
    "    logits = outputs.logits        # [B, T_total, V]\n",
    "\n",
    "    # 完全照源码对齐：只取与 labels 等长的尾部，做自回归 shift\n",
    "    #logits = logits[:, -labels.size(1):, :]              # [B, T, V]\n",
    "    shift_logits = logits[..., :-1, :].contiguous()      # [B, T-1, V]\n",
    "    shift_labels = labels[..., 1:].contiguous()          # [B, T-1]\n",
    "\n",
    "    loss = loss_fct(\n",
    "        shift_logits.view(-1, shift_logits.size(-1)),\n",
    "        shift_labels.view(-1),\n",
    "    )\n",
    "\n",
    "    epoch_loss += loss.item()\n",
    "    pred = outputs.logits.argmax(dim=-1)\n",
    "    train_acc += compute_metrics((pred, labels))[\"rougeL\"]  # You can use rouge1,\n",
    "    # Clear cache to avoid OOM\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training loss: 0.000 - Train Acc: 0.000:   0%|          | 0/44376 [01:19<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[11], line 33\u001B[0m\n\u001B[0;32m     31\u001B[0m train_acc \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m compute_metrics((pred, labels))[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrougeL\u001B[39m\u001B[38;5;124m\"\u001B[39m]  \u001B[38;5;66;03m# You can use rouge1,\u001B[39;00m\n\u001B[0;32m     32\u001B[0m \u001B[38;5;66;03m# Clear cache to avoid OOM\u001B[39;00m\n\u001B[1;32m---> 33\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcuda\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mempty_cache\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     34\u001B[0m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39msynchronize()\n",
      "File \u001B[1;32mD:\\toolkit\\anaconda\\envs\\torch\\lib\\site-packages\\torch\\cuda\\memory.py:224\u001B[0m, in \u001B[0;36mempty_cache\u001B[1;34m()\u001B[0m\n\u001B[0;32m    213\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Release all unoccupied cached memory currently held by the caching\u001B[39;00m\n\u001B[0;32m    214\u001B[0m \u001B[38;5;124;03mallocator so that those can be used in other GPU application and visible in\u001B[39;00m\n\u001B[0;32m    215\u001B[0m \u001B[38;5;124;03m`nvidia-smi`.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    221\u001B[0m \u001B[38;5;124;03m    more details about GPU memory management.\u001B[39;00m\n\u001B[0;32m    222\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    223\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_initialized():\n\u001B[1;32m--> 224\u001B[0m     \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_C\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_cuda_emptyCache\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 11
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
